-*- mode: Org; org-todo-keywords '((sequence "TODO" "DOING" "DONE"))
#+begin_html
<embed src="defone-logo.svg" width=400 type="image/svg+xml" />
#+end_html

(defone): "The Habitable Phone OS"

* Intro

Current phone operating systems that treat apps as monoliths.  The
goal of this project is (eventually) to replace this with small
general-purpose components which can be strung together according to
user needs.  

** How?

Using CSP, which for the purpose of this discussion I will summarize
as "like Unix pipes but on steroids".  CSP is about Communicating
Sequential Processes, and the way they communicate is by being
connected together with channels and passing messages on those
channels.  

** No, really, _how_?

Take an Android phone, install a proper Linux distribution and 
a Clojure installation on it, then write/join up components using
core.async.  Input will be from RIL and the internet and tslib, 
output will be via OpenGL(ES)

*** Why Android?

Mostly because it's there.  We piggyback on android to get the kernel
and the drivers for stuff like RIL (the radio/telephony) and wireless
and audio and bluetooth.  We ignore (maybe even delete, eventually)
all the Dalvik stuff.

*** Why not Clojure-on-Dalvik?

No particularly strong reason except that (1) dynamic compilation
looks complicated, (2) putting our own real Linux on instead gives us
a degree of future-proofing by making it more portable

- there are other Linux-based phones that aren't Android or have only
  vestigial Android (Firefox OS, Ubuntu Touch)

- there are ARM Linux devices that aren't even phones (Raspberry Pi
  being the obvious one, plus Sheevaplugs and all that stuff)

The biggest downside of doing it this way, at least that I have so
far noted, is that we don't get Android accelerated display drivers.

*** Why not start with B2G/Firefox OS?

Because I tried building it for my device (Galaxy Nexus) and sheesh is
it complicated.  Modularity FTW: if we can run (defone) on any
powerful-enough hardware that runs a rooted Android, that gives us
many more potential target hardware devices.

* Current status / TODO

(Very) rough, (not at all) ready.  

Next tasks: 
- chop up defone.core into separate namespaces
- make the touch event data drive the graphics being generated

** Infrastrcuture
*** chroot (Debian) / more or less functional

I've built a chroot with Debian in it and some make targets to get it
onto my phone.  I have tested this works with CyanogenMod (which is
what my phone is running) and with the vanilla Android that the
emulator runs.  I advise against trying to run it on the emulator
unless you have a near-infinite amount of spare time to wait for it,
though.  By which I mean to say, an actual phone will be much faster.

    $ sudo -E make -C rootfs

with your phone connected on USB.  This 

- creates a Debian chroot
- install rsync on the phone with adb push
- copies the chroot onto the phone
- "ssh root@the-phone make -C /defone/stage2" - connect to the phone
  and kick off the next stage

As with anything based on Make, it may not work.  

*** graphics (Mesa) / working but slow

I spent a good while chasing down various rabbit holes in the hope of
getting hw-accelerated GL on my particular phone (Galaxy Nexus).
Eventually I wrote that off as an exercise for Future Me and now just
use Mesa instead.

Builing Mesa is one of the targets of the stage2 build (i.e. it is
built natively by the phone).  Although the Makefile that builds and
installs the chroot is supposed to kick off a stage2 build, it
currently fails due to some search path problem I haven't taken the
time to figure out yet.  ssh into the phone then run

    # make -C /defone/stage2

to restart it by hand

Allegedly it builds with llvmpipe, but casual use says it's no faster than
boring old softpipe.  But that could easily be because I haven't
figured out how to enable llvmpipe properly

#+BEGIN EXAMPLE
root@localhost:~/src# EGL_PLATFORM=fbdev GALLIUM_DRIVER=llvmpipe 
 LD_LIBRARY_PATH=/usr/local/lib EGL_LOG_LEVEL=debug  ./eglinfo
libEGL debug: Native platform type: fbdev (environment overwrite)
libEGL debug: EGL search path is /usr/local/lib/egl
libEGL debug: added /usr/local/lib/egl/egl_gallium.so to module array
libEGL debug: added egl_dri2 to module array
libEGL debug: dlopen(/usr/local/lib/egl/egl_gallium.so)
libEGL info: use FBDEV for display 0x3
libEGL debug: the best driver is Gallium
EGL API version: 1.4
EGL vendor string: Mesa Project
EGL version string: 1.4 (Gallium)
EGL client APIs: OpenGL OpenGL_ES OpenGL_ES2 
EGL extensions string:
    EGL_MESA_screen_surface EGL_KHR_image_base EGL_KHR_reusable_sync
    EGL_KHR_fence_sync EGL_KHR_surfaceless_context
Configurations:
     bf lv colorbuffer dp st  ms    vis   cav bi  renderable  supported
  id sz  l  r  g  b  a th cl ns b    id   eat nd gl es es2 vg surfaces 
---------------------------------------------------------------------
0x01 32  0  8  8  8  8  0  0  0 0 0x00SG      a  y  y  y     win,pb,scrn
0x02 32  0  8  8  8  8 16  0  0 0 0x00SG      a  y  y  y     win,pb,scrn
0x03 32  0  8  8  8  8 32  0  0 0 0x00SG      a  y  y  y     win,pb,scrn
0x04 32  0  8  8  8  8 24  8  0 0 0x00SG      a  y  y  y     win,pb,scrn
0x05 32  0  8  8  8  8 24  0  0 0 0x00SG      a  y  y  y     win,pb,scrn
Number of Screens: 1

Screen 0 Modes:
  id  width height refresh  name
-----------------------------------------
0x01   720   1280   60.000  Current Mode
#+END EXAMPLE

*** JDK, Leiningen / installed

in stage2 build

*** Actual Clojure code

The stage2 build copies /defone/defone/ onto the device.  This is 
a bog-standard clojure project created by "lein new"

It's not autostarted yet: probably we will spin up a headless nrepl
on port 9990 (  âœ† / U+2706 /Telephone Location Sign) and maybe have it
respawn on exit.

It is a bit sucky having everything in one clojure project because it
means bringing the whole platform down whenever we want to add another
library - or whenever for any reason we get the JVM into a bad state
and need to restart it.  Definitely a topic we will need to revisit.

*** Standalone hacking

It is a goal that we can develop directly on the device without being
dependent on some other machine with an SDK/special software
installed.  Since we have no plans currently to support programming without a
keyboard, we will at least be dependent on being able to connect a
keyboard or a device that has one attached.

I'm not yet sure what this should look like.  I guess we want shell,
repl, editor all exported via a web interface so we could point any
browser at the phone and hack.  Bonus points if it was reattaching to
an existing session instead of having to recreate context on each request.

Would we expect it to be better than emacs?  Would we expect it to be
emacs?  Light table?

*** Touchscreen

We have succeeded in opening the touchscreen device, getting bytes out
of it, and turning those into maps that model the badly named struct
input_event.  To do:

- work out which input device is the touchscreen (currently hardcoded)

- aggregate each bunch of events between successive 'sync' events
  into a single thing that represents an actual touch

- process those to be meaningful
  - normalise co-ords
  - identify gestures (swipe, drag, etc)
  - etc

- write a finger painting demo

*** Gestures

The sticking point for gesture recognition so often seems to be
distinguishing between receivers of an in-progress gesture: for
example, are you trying to paint something or are you trying to swipe
the paint app offscreen and do something else.

One approach would be to say that *everything* goes to the foreground
UI except maybe for some strictly limited set of events (say, touches
that start/end in the bottom 10% of the screen) then there simply
isn't a conflict.  Or even use a hardware button for task
switching/access to the "window manager"

One idea worth exploring is that we might model "flings" (traces
where the finger velocity at touch-end is non-zero) by sending fake
movement events after touch-end that "decelerate".  I guess we'd have
to have some concept of 'momentum' so that the app would tell the
channel the weight of the object being flung and bigger UI elements
would take correspondingly longer to slow down

*** IP Networking
   
Android handles this, mostly no need to get involved.  But we could
use some way of updating resolv.conf when the network connectivity changes.

*** GPS

Not even looked at this yet, but I think it's part of RIL

*** Telephony

RIL looks at least semi-documented.  Write some stuff to channelise it

*** Camera

Not looked at this yet either


** Apps and API
*** data sources

I see no reason not to use sqlite3 for local data providers as android
does.  We can issue queries against the data source that return a
channel, and a message on that channel for each result row

Binary chunks over channels for network IO is also conceivable.  For
structured data we should be able to interpose a parser into the
channel flow so that our consumer gets a json-style dictionary or an
html element+kids or something meaningful like that on each read.

*** Graphics

We have drawn a rotating triangle using GLES via JNA.  This took
longer than anticipated because anything involving FFI always takes
longer than anticipated. 

We have a rudimentary scene graph in an atom and an opengl thread that
gets woken up when the grah changes

defone.ui=> (swap! the-scene update-in [2 1] #(+ 0.05 %))
;; rotate by 0.05 radians

**** Widgets

What do we need?
- buttons (emit events on some channel when pushed)
- one-row-per-event table, connected to input channel
- one-col-per-event tables?
- x/y graphs, maybe

Should UI widgets be able to reconfigure themselves dynamically
(e.g. send a message to an x/y graph saying "now rescale and show y
values up to 600") or is that state we'd be better off without and
should the parent widget destroy and create afresh?


*** Audio IO

ALSA is standard Linux stuff.  Need to find out if channels are good
for bulk audio data (maybe in 8k blocks or something) or if they
would be best reserved for signalling and let the actual audio happen
out of bound.

